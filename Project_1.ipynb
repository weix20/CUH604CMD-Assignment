{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2fuh/jC5k+QjvoVxpMyf0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weix20/CUH604CMD-Assignment/blob/main/Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is a binary classification problem (label ∈ {0,1}).\n",
        " - We use a Feedforward Neural Network (FNN: Dense + ReLU hidden, Sigmoid output)\n",
        " Trained with Adam + binary_crossentropy;\n",
        " - features are standardized.\n",
        "\n"
      ],
      "metadata": {
        "id": "ueHtIj213uUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Automatically load the \"/content/data.csv\" file\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Show the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Prepare dataset\n",
        "# Separate features and labels\n",
        "X = df.drop(columns=['y'])  # Features\n",
        "y = df['y']  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% for training, 20% testing)\n",
        "# The random_state ensures that the split is reproducible\n",
        "# stratify=y keeps class ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=24, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize the data (important for neural networks)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build a Feedforward Neural Network (FNN)\n",
        "def create_model(layers_count=3, neurons_per_layer=64, dropout_rate=0.2, learning_rate=0.001):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Input layer matches the number of features\n",
        "    model.add(layers.InputLayer(shape=(X_train.shape[1],)))\n",
        "\n",
        "\n",
        "    # Hidden layers: Dense + ReLU + Dropout\n",
        "    for _ in range(layers_count):\n",
        "        model.add(layers.Dense(neurons_per_layer, activation='relu'))\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer: 1 neuron with Sigmoid for binary classification\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile with Adam optimizer and binary crossentropy loss\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy',  # Binary classification loss\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define three models with different hyperparameters\n",
        "# Model 1: Default architecture\n",
        "model1 = create_model(layers_count=3, neurons_per_layer=64, dropout_rate=0.2, learning_rate=0.001)\n",
        "\n",
        "# Model 2: Increased neurons and layers\n",
        "model2 = create_model(layers_count=4, neurons_per_layer=128, dropout_rate=0.3, learning_rate=0.0005)\n",
        "\n",
        "# Model 3: Different architecture with fewer layers and neurons\n",
        "model3 = create_model(layers_count=2, neurons_per_layer=32, dropout_rate=0.1, learning_rate=0.002)\n",
        "\n",
        "# To add early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train all models\n",
        "print(\"\\n===== TRAINING Model 1 =====\")\n",
        "history1 = model1.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
        "                      callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "print(\"\\n===== TRAINING Model 2 =====\")\n",
        "history2 = model2.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
        "                      callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "print(\"\\n===== TRAINING Model 3 =====\")\n",
        "history3 = model3.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
        "                      callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "# Evaluate all models on the test set\n",
        "print(\"\\n----- EVALUATING on TEST -----\")\n",
        "loss1, accuracy1 = model1.evaluate(X_test, y_test)\n",
        "loss2, accuracy2 = model2.evaluate(X_test, y_test)\n",
        "loss3, accuracy3 = model3.evaluate(X_test, y_test)\n",
        "\n",
        "# Predict the results on the test set\n",
        "proba1 = model1.predict(X_test, verbose=0).ravel()\n",
        "proba2 = model2.predict(X_test, verbose=0).ravel()\n",
        "proba3 = model3.predict(X_test, verbose=0).ravel()\n",
        "\n",
        "y_pred1 = (model1.predict(X_test) > 0.5).astype(int)  # Convert binary output to 0 or 1\n",
        "y_pred2 = (model2.predict(X_test) > 0.5).astype(int)\n",
        "y_pred3 = (model3.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "# Calculate precision, recall, F1-score, and ROC AUC\n",
        "precision1 = precision_score(y_test, y_pred1)\n",
        "recall1 = recall_score(y_test, y_pred1)\n",
        "f1_1 = f1_score(y_test, y_pred1)\n",
        "roc_auc1 = roc_auc_score(y_test, y_pred1)\n",
        "\n",
        "precision2 = precision_score(y_test, y_pred2)\n",
        "recall2 = recall_score(y_test, y_pred2)\n",
        "f1_2 = f1_score(y_test, y_pred2)\n",
        "roc_auc2 = roc_auc_score(y_test, y_pred2)\n",
        "\n",
        "precision3 = precision_score(y_test, y_pred3)\n",
        "recall3 = recall_score(y_test, y_pred3)\n",
        "f1_3 = f1_score(y_test, y_pred3)\n",
        "roc_auc3 = roc_auc_score(y_test, y_pred3)\n",
        "\n",
        "# Print results for each model\n",
        "print(f\"Model 1 - Loss: {loss1:.4f}, Accuracy: {accuracy1:.4f}, Precision: {precision1:.4f}, Recall: {recall1:.4f}, F1-score: {f1_1:.4f}, ROC AUC: {roc_auc1:.4f}\")\n",
        "print(f\"Model 2 - Loss: {loss2:.4f}, Accuracy: {accuracy2:.4f}, Precision: {precision2:.4f}, Recall: {recall2:.4f}, F1-score: {f1_2:.4f}, ROC AUC: {roc_auc2:.4f}\")\n",
        "print(f\"Model 3 - Loss: {loss3:.4f}, Accuracy: {accuracy3:.4f}, Precision: {precision3:.4f}, Recall: {recall3:.4f}, F1-score: {f1_3:.4f}, ROC AUC: {roc_auc3:.4f}\")\n",
        "\n",
        "# Choose the best model based on accuracy\n",
        "best_model = None\n",
        "best_accuracy = max(accuracy1, accuracy2, accuracy3)\n",
        "\n",
        "if best_accuracy == accuracy1:\n",
        "    best_model = model1\n",
        "elif best_accuracy == accuracy2:\n",
        "    best_model = model2\n",
        "else:\n",
        "    best_model = model3\n",
        "\n",
        "# Final performance evaluation\n",
        "print(f\"The best model is Model {['1', '2', '3'][[accuracy1, accuracy2, accuracy3].index(best_accuracy)]} with accuracy: {best_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "IGzy-SF_g2OZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430c1cf6-69ce-4495-e7b1-b5c4ab7797a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   x0  x1  x2  x3  x4  x5  x6  x7  x8  x9  x10  x11  x12  x13  x14  y\n",
            "0   0   1   0   0   0   0   0   0   0   0    0    0    0    0    0  1\n",
            "1   0   0   1   0   0   0   0   0   0   0    0    0    0    0    0  0\n",
            "2   0   0   0   1   0   0   0   0   0   0    0    0    0    0    0  1\n",
            "3   0   0   0   0   1   0   0   0   0   0    0    0    0    0    0  0\n",
            "4   0   0   0   0   0   1   0   0   0   0    0    0    0    0    0  1\n",
            "\n",
            "===== TRAINING Model 1 =====\n",
            "Epoch 1/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.5019 - loss: 0.7005 - val_accuracy: 0.5009 - val_loss: 0.6939\n",
            "Epoch 2/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.5033 - loss: 0.6943 - val_accuracy: 0.4966 - val_loss: 0.6934\n",
            "Epoch 3/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5133 - loss: 0.6932 - val_accuracy: 0.5026 - val_loss: 0.6936\n",
            "Epoch 4/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.5084 - loss: 0.6932 - val_accuracy: 0.4971 - val_loss: 0.6939\n",
            "Epoch 5/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5201 - loss: 0.6926 - val_accuracy: 0.4988 - val_loss: 0.6937\n",
            "Epoch 6/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5172 - loss: 0.6921 - val_accuracy: 0.4869 - val_loss: 0.6943\n",
            "Epoch 7/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5173 - loss: 0.6923 - val_accuracy: 0.4928 - val_loss: 0.6940\n",
            "\n",
            "===== TRAINING Model 2 =====\n",
            "Epoch 1/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5007 - loss: 0.7002 - val_accuracy: 0.4940 - val_loss: 0.6936\n",
            "Epoch 2/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4984 - loss: 0.6951 - val_accuracy: 0.4947 - val_loss: 0.6935\n",
            "Epoch 3/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.5125 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
            "Epoch 4/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.5038 - loss: 0.6937 - val_accuracy: 0.4928 - val_loss: 0.6936\n",
            "Epoch 5/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.5070 - loss: 0.6933 - val_accuracy: 0.4931 - val_loss: 0.6936\n",
            "Epoch 6/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5004 - loss: 0.6934 - val_accuracy: 0.4948 - val_loss: 0.6940\n",
            "Epoch 7/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5068 - loss: 0.6931 - val_accuracy: 0.4980 - val_loss: 0.6935\n",
            "Epoch 8/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.5159 - loss: 0.6922 - val_accuracy: 0.4915 - val_loss: 0.6941\n",
            "\n",
            "===== TRAINING Model 3 =====\n",
            "Epoch 1/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4993 - loss: 0.7023 - val_accuracy: 0.5024 - val_loss: 0.6936\n",
            "Epoch 2/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5091 - loss: 0.6936 - val_accuracy: 0.4989 - val_loss: 0.6936\n",
            "Epoch 3/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5144 - loss: 0.6925 - val_accuracy: 0.4940 - val_loss: 0.6946\n",
            "Epoch 4/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5232 - loss: 0.6921 - val_accuracy: 0.4881 - val_loss: 0.6946\n",
            "Epoch 5/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5153 - loss: 0.6922 - val_accuracy: 0.4873 - val_loss: 0.6957\n",
            "Epoch 6/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5224 - loss: 0.6914 - val_accuracy: 0.4904 - val_loss: 0.6958\n",
            "Epoch 7/50\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5246 - loss: 0.6913 - val_accuracy: 0.4849 - val_loss: 0.6967\n",
            "\n",
            "----- EVALUATING on TEST -----\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4877 - loss: 0.6936\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4973 - loss: 0.6933\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4932 - loss: 0.6933\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Model 1 - Loss: 0.6934, Accuracy: 0.4966, Precision: 0.4969, Recall: 0.5417, F1-score: 0.5183, ROC AUC: 0.4966\n",
            "Model 2 - Loss: 0.6932, Accuracy: 0.5000, Precision: 0.5000, Recall: 0.3811, F1-score: 0.4325, ROC AUC: 0.5000\n",
            "Model 3 - Loss: 0.6936, Accuracy: 0.4989, Precision: 0.4992, Recall: 0.6629, F1-score: 0.5695, ROC AUC: 0.4989\n",
            "The best model is Model 2 with accuracy: 0.5000\n"
          ]
        }
      ]
    }
  ]
}