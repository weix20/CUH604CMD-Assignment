{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6m6SzmjpV8v"
   },
   "source": [
    "# Project 2 — Academic Tutor LM for 5000CMD (Theory of Computation)\n",
    "\n",
    "**Goal**  \n",
    "Fine-tune a small GPT model (**distilgpt2**) on the 5000CMD course pages  \n",
    "so it can produce short and clear explanations (like DFAs, etc.).\n",
    "\n",
    "**Steps**\n",
    "1. Grab and clean text from the course website  \n",
    "2. Make a dataset and split into train/val/test  \n",
    "3. Load distilgpt2 and its tokenizer  \n",
    "4. Fine-tune it with a PyTorch training loop  \n",
    "5. Check results with loss/perplexity + run a sample generation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyEFEFw7w6Z0",
    "outputId": "39b9f206-313b-49c6-fd61-d5310f2046a3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up the environment so Colab has the right versions of the libraries\n",
    "!pip -q install --no-cache-dir \\\n",
    "  \"transformers==4.43.4\" \\\n",
    "  \"datasets==2.20.0\" \\\n",
    "  \"accelerate==0.33.0\" \\\n",
    "  \"huggingface-hub>=0.34.0,<1.0\" \\\n",
    "  \"beautifulsoup4==4.12.2\" \\\n",
    "  \"lxml==5.2.2\" \\\n",
    "  \"unidecode==1.3.8\" \\\n",
    "  \"tldextract==5.1.2\" \\\n",
    "  \"pyarrow<17\" \"fsspec==2024.5.0\" \\\n",
    "  \"numpy==1.26.4\" \"pandas==2.2.2\"\n",
    "\n",
    "# Import libraries\n",
    "import os, re, json, time, hashlib, textwrap\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urldefrag\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import tldextract\n",
    "\n",
    "# Import the NLP and dataset tools we need\n",
    "import transformers, datasets, accelerate, huggingface_hub\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Version check\n",
    "print(\"✅ Environment ready:\")\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"huggingface-hub:\", huggingface_hub.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4qsoFCFdE9D",
    "outputId": "23f0c42c-1232-401a-e399-30154084c008"
   },
   "outputs": [],
   "source": [
    "# --- Data crawl config ---\n",
    "BASE_URL = \"https://github.coventry.ac.uk/pages/ab3735/5000CMD/\"\n",
    "SAVE_DIR = Path(\"/content/5000cmd_data\")\n",
    "RAW_DIR  = SAVE_DIR / \"raw_html\"\n",
    "CLEAN_DIR = SAVE_DIR / \"clean\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_PAGES = 60    # don’t crawl too many\n",
    "REQUEST_TIMEOUT = 15  # timeout per reques (seconds)\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"Colab data prep for coursework\"})\n",
    "\n",
    "def same_site(url, base):\n",
    "    return tldextract.extract(url).registered_domain == tldextract.extract(base).registered_domain\n",
    "\n",
    "def should_visit(url):\n",
    "    # only keep same-site HTML pages; skip pdf/images/css/js/etc.\n",
    "    if not same_site(url, BASE_URL):\n",
    "        return False\n",
    "    if any(url.lower().endswith(ext) for ext in (\".pdf\", \".zip\", \".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".css\", \".js\")):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def fetch(url):\n",
    "   # just fetch the page content (raise error if status not ok)\n",
    "    r = SESSION.get(url, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def extract_main_text(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # remove menus, scripts, styles so we only keep useful content\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    for sel in [\"nav\", \"header\", \"footer\", \"aside\"]:\n",
    "        for t in soup.select(sel):\n",
    "            t.decompose()\n",
    "\n",
    "    # try to grab <main> or <article>, else just take <body>\n",
    "    main = soup.select_one(\"main\") or soup.select_one(\"article\") or soup.body or soup\n",
    "    text = \"\\n\".join(p.get_text(\" \", strip=True) for p in main.find_all([\"h1\",\"h2\",\"h3\",\"p\",\"li\",\"pre\",\"code\"]))\n",
    "    title = (soup.title.get_text(\" \", strip=True) if soup.title else \"\").strip()\n",
    "    return title, text\n",
    "\n",
    "def normalize(text):\n",
    "    # clean up: fix whitespace, ascii-fy, drop super short/boilerplate lines\n",
    "    text = unidecode(text)\n",
    "    boilerplate = [\n",
    "        \"back to top\", \"coventry university\", \"github pages\", \"privacy\", \"cookies\"\n",
    "    ]\n",
    "    # Clean up text lines:\n",
    "    lines = []\n",
    "    for ln in re.split(r\"\\s*\\n\\s*\", text):\n",
    "        ln = re.sub(r\"\\s+\", \" \", ln).strip()\n",
    "        if len(ln) < 20:        # skip lines that are too short\n",
    "            continue\n",
    "        if any(bp in ln.lower() for bp in boilerplate):\n",
    "            continue\n",
    "        lines.append(ln)\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "def crawl(base_url=BASE_URL, max_pages=MAX_PAGES):\n",
    "    visited, queue = set(), [base_url]\n",
    "    docs = []\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        url = queue.pop(0)\n",
    "        url, _ = urldefrag(url)  # remove the #section part from URL\n",
    "        if url in visited or not should_visit(url):\n",
    "            continue\n",
    "        try:\n",
    "            html = fetch(url)\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ skip:\", url, \"-\", e)\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "         # save raw html\n",
    "        (RAW_DIR / f\"{hashlib.md5(url.encode()).hexdigest()}.html\").write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "        title, raw_text = extract_main_text(html)\n",
    "        clean_text = normalize(raw_text)\n",
    "\n",
    "        if clean_text:\n",
    "            docs.append({\n",
    "                \"url\": url,\n",
    "                \"title\": title or \"Untitled\",\n",
    "                \"text\": clean_text\n",
    "            })\n",
    "            # quick progress log\n",
    "            print(f\"✅ {len(visited):>3}/{max_pages}  {title[:60]}  ({len(clean_text)} chars)\")\n",
    "\n",
    "        # find new links on the same site\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            nxt = urljoin(url, a[\"href\"])\n",
    "            nxt, _ = urldefrag(nxt)\n",
    "            if nxt not in visited and should_visit(nxt):\n",
    "                queue.append(nxt)\n",
    "\n",
    "    return docs\n",
    "\n",
    "docs = crawl()\n",
    "len(docs), docs[0] if docs else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wt7pBGFHdTWN",
    "outputId": "991d31b9-9f0f-4e72-855c-ef6e7464d45c"
   },
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "seen = set()\n",
    "unique_docs = []\n",
    "for d in docs:\n",
    "    h = hashlib.sha1(d[\"text\"].encode()).hexdigest()\n",
    "    if h in seen:\n",
    "        continue\n",
    "    seen.add(h)\n",
    "    unique_docs.append(d)\n",
    "\n",
    "\n",
    "print(f\"📦 total {len(docs)} docs, after dedup {len(unique_docs)} docs\")\n",
    "\n",
    "# save dataset to JSONL (one record per line, good for reloading later)\n",
    "jsonl_path = CLEAN_DIR / \"5000cmd_clean.jsonl\"\n",
    "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for d in unique_docs:\n",
    "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# also save as CSV (optional, easier to open in Excel)\n",
    "import csv\n",
    "csv_path = CLEAN_DIR / \"5000cmd_clean.csv\"\n",
    "with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"url\",\"title\",\"text\"])\n",
    "    w.writeheader()\n",
    "    for d in unique_docs:\n",
    "        w.writerow(d)\n",
    "\n",
    "print(\"✅ Saved to:\", jsonl_path, \"and\", csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0w1u4Ch5dVGc",
    "outputId": "400caf1a-7761-44a0-a81c-e21897ac0f83"
   },
   "outputs": [],
   "source": [
    "def build_examples(rows):\n",
    "    examples = []\n",
    "    for d in rows:\n",
    "        # put the title at the top of the text （helps the model learn the topic)\n",
    "        merged = f\"# {d['title']}\\n\\n{d['text']}\".strip()\n",
    "        examples.append({\"text\": merged, \"meta_url\": d[\"url\"]})\n",
    "    return examples\n",
    "\n",
    "examples = build_examples(unique_docs)\n",
    "print(\"Number of samples:\", len(examples))\n",
    "print(\"Preview:\\n\", textwrap.shorten(examples[0][\"text\"], width=300, placeholder=\"…\"))\n",
    "\n",
    "# make a Hugging Face Dataset and split it into train/val/test\n",
    "ds_all = Dataset.from_list(examples)\n",
    "# split the dataset: 80% train   10% validation   10% test\n",
    "tmp = ds_all.train_test_split(test_size=0.2, seed=413)\n",
    "ds_val_test = tmp[\"test\"].train_test_split(test_size=0.5, seed=413)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": tmp[\"train\"],\n",
    "    \"validation\": ds_val_test[\"train\"],\n",
    "    \"test\": ds_val_test[\"test\"]\n",
    "})\n",
    "\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513,
     "referenced_widgets": [
      "cac660238c014933ac96f29d79c55d54",
      "76117d2910aa4dbca286b70b85ca0aa5",
      "5ad12ab658a84d14be4e7cd4ce7f30dc",
      "79e7624050ea49759ebcbf956fe15eb6",
      "64fc21a70e614725ab1c7b91129e409c",
      "9c609047fde9453eb2dc3fe77e58d15f",
      "a4ab696284744c1abcabc4d9bd5dd077",
      "3aa3d623659f4b9b81e03cd4aa7ceff0",
      "c0fcba10136142afbdc02b5f47081471",
      "f250e756fe5549048ccbb7e7dbcdf18f",
      "f8d1842c621f4cd9811d84126a24c430",
      "014718fed8d642999fccfbeff96236fc",
      "0d59bd1926c1429b8b6237464fab8015",
      "876bfeaec2324a29b1e7a89606ec9f13",
      "a385302b793d431bb7308c302c4f6cff",
      "250da3294d124976afd328b8ff7a21fe",
      "eb21f98cb07c403dadf8bff5d6765796",
      "5db5dfa8422a4db58629c67a53250018",
      "e80d9c83e709488ea7378d91ab000570",
      "f5bb02153f534d6e9d582c183a5cbbd8",
      "e6749ce7a3db46a09fa292bb9c12d46b",
      "3a10c90d70754480bc64525ce17bb603",
      "92e4c7fbab6b4adb9198c259f06c6890",
      "575124ca272a4952a06ec5b039e2b63f",
      "2b38ac4b7ef7452eb9cc343a66b187b2",
      "b21ea12051ca45dba9dd8ff090d3140e",
      "cf53501b301448b9b57c986b1054d3ce",
      "1c7d316adf534dfcbbd958e18ab9c311",
      "10854be67d7d4fa2ba12619ff723c907",
      "a5efc92869764921b36ff216faf81845",
      "e798fed6f7514515ab9ebf6874c16ecc",
      "f209c6eeb35943558e2d44c37c0a1d78",
      "78ef879f8ee54defb998f53a9ee148b4"
     ]
    },
    "id": "oKTnOmXYdXnI",
    "outputId": "5978ba7c-ef71-4f56-9108-489ab6140e9f"
   },
   "outputs": [],
   "source": [
    "def char_len(example):\n",
    "    # add a new field: how many characters each text has\n",
    "    return {\"n_chars\": len(example[\"text\"])}\n",
    "\n",
    "ds_with_len = dataset_dict.map(char_len)\n",
    "print(\"Train size:\", len(ds_with_len[\"train\"]))\n",
    "print(\"Val size:\", len(ds_with_len[\"validation\"]))\n",
    "print(\"Test size:\", len(ds_with_len[\"test\"]))\n",
    "\n",
    "# show the first two samples from each split (train/val/test)\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"\\n=== {split.upper()} preview ===\")\n",
    "    for i in range(min(2, len(ds_with_len[split]))):\n",
    "        ex = ds_with_len[split][i]\n",
    "        print(\"URL:\", ex[\"meta_url\"])\n",
    "        print(textwrap.shorten(ex[\"text\"], width=280, placeholder=\"…\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "a5a9a56898d64cfb884d5e81091725b7",
      "2dd11289695b416cbefcf563926d1e32",
      "59d88ba06e1c47f880e8f00a91b09d50",
      "00f2d043083248f1ba1408366859aeca",
      "76c9fd9d82a54f66ae8472abc09474c7",
      "57dd6dc6d1e74b1a9da8f5efc987a429",
      "c90a3ef86d1a4470ba9b97fd5bb325a6",
      "bf2dd8f56b8e430dbea89381bd05a8d3",
      "6fd2ba6fd4a645efa0f81344f7f37d41",
      "d50cfc499ead4172ae2f218318983387",
      "aadea202f8c94b1cad25be7a324e548d",
      "28ddf9da11c24fb2aa53625760952339",
      "667b79b78ed847158c026325373282ad",
      "bfa5ee02c2f349f1b060b7d3e14e7eea",
      "1102da9c2c994a6caec9d3cdba3dfce9",
      "b23c1521179741f2853cfe6cbd9f21fa",
      "0727d54089cb4999a7e67c8aa6e21ae7",
      "39daf33ef51148cf8f08a13a3f3fe156",
      "515a4665e706441989ab0190d40c8503",
      "55162f4a7e944df89fceede22dd2655c",
      "9e2a41cd26c24c8f82abd0206e57d777",
      "cea39905974a4c2585d289800921de2f",
      "d0113d1d650b48fcbdd5695b6dd76e33",
      "f297a52fc6074e649c68b39ccf319ec0",
      "1ae401d07591434cbf3e370dd357070d",
      "eedcac1236f6484bb36c0d716f79bcb5",
      "36e950ed6d7a45bf84ca0816da35552d",
      "b46ef807903e4bc4b64c4179bd10170f",
      "76f79b8ea97a4fd6978b991502479dbe",
      "e4841c9a904e475783d476d59c2e5dac",
      "2018503e618745cf8d4aadf714fbcc79",
      "6c3ed3b9f01a442e9e012653a54407fc",
      "d085b0ae92364addb08c3fa5a422cc10"
     ]
    },
    "id": "yMcHMjRydYEV",
    "outputId": "ce34c096-e908-4280-c0ce-72eb0e869658"
   },
   "outputs": [],
   "source": [
    "DATASET_DISK_DIR = str(SAVE_DIR / \"hf_dataset\")\n",
    "dataset_dict.save_to_disk(DATASET_DISK_DIR)\n",
    "print(\"📁 Dataset saved to:\", DATASET_DISK_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3zDP4Xqd7V4",
    "outputId": "6d2d747c-a189-404f-8617-8f02d4fe2dd9"
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"/content/5000cmd_data/hf_dataset\")\n",
    "print(dataset)\n",
    "\n",
    "# look at one sample from the training set\n",
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNVHzooufH6h",
    "outputId": "3360a510-accf-4cba-8580-57082c1568c9"
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"/content/5000cmd_data/hf_dataset\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzAch5ZpfJpQ",
    "outputId": "97731d07-cb3c-48c8-e57b-17db9cb43a4e"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500,
     "referenced_widgets": [
      "86259a4ee5b44bda90a2f8cb1b7904bf",
      "48a69b21ef694ebe9ccba3bd45dfaf5e",
      "b891e2dd92284983936ba31cfc0f590c",
      "ecc48e16818a4b98b89ab0786162c90d",
      "96eb5be5e5d643489986213056c3e4f9",
      "61899d146d9d4d96a4ae8bbfbbbfb904",
      "7363268eabb6430889b1b2276def60b0",
      "781e033019e3480a9d118e4906f42802",
      "a2e373f6eb184e8b897b15c894c08bd8",
      "c1b79ee8b5ec4f58a41b144bec301e84",
      "2bdefe8e787f40319fcf296ce5be2f8a",
      "cd6f78f9582040e280e2004c014d8396",
      "7b2cb9e8b213446294477a6aa63985f8",
      "45d6f660537748a293bfcdfdbed67910",
      "22148ce5518e4d7b8a3996292c510e88",
      "a96046342cf0423db55168fcb2442206",
      "91c23805660e43168ca345417ca05c93",
      "10c8b853f70c46d182ecbc3a426ebabb",
      "4280f9a3b2a549aa805bd4deedebab7b",
      "cf584868ab8c429aa201fd701b1cdd55",
      "3250cbc905c249f28d9066cc3f289e86",
      "f8e95d11e2d74e6a8ea1a4305d82fdb1",
      "389c227cec12403ea9841d454a508464",
      "4cd4ace0014f4206a0477c7a46805229",
      "54e3f9dd5e5c4d8fa13e8aa95a837acd",
      "10c24bf4f5804560a71606fad3b97202",
      "cc066d0c81cc4b03948727323594339d",
      "5367d4ff7481450b8e21de3f75af180c",
      "b33852422a9f4cedb19fd6ffc729cb2a",
      "4a40dee7625545a194e9e20565a29021",
      "ccdf746f099249dfa49b6704c101a329",
      "d4e8c4e5a291415985c55366f35fda8f",
      "8ba3d0986b6e4e0a9923a71d7da6fc79",
      "60cdc4fa3df24324b360cfc1c5872d3d",
      "d76c3adb2c1a41ab9573d98f784ecc95",
      "b06100d187f043cd864baab75ba31013",
      "ce29e05ea1a642979bd80fbf82cd24ce",
      "746ad8a48e3048a5b16162aa08593661",
      "a1642af0b44a46a8a764a82ecd1534b0",
      "300a8a5db98540ed9303b0bc084ccff6",
      "45c1727fdbef47ecbf4b376dcca5f617",
      "78381ebc16924507b289edb95fdc67e3",
      "eb860d2b7b114ec9aa7e936192057206",
      "b5f7cd086f6a4362b0b97e4b7aa37be2",
      "84a3e08d0bc242829cbaebe4c428b236",
      "f1df52441d094a28b2680ea2504cebf9",
      "55fa134193334cd2a08e1cbe8e4d6c2e",
      "b1faccdeb6fa4e7bbeec2230cc3f4015",
      "0e360aaeef314e37a004b0d9f8a61f7b",
      "025c393dd96f4d9db47b219977f72d08",
      "0c5bba2fa8de4c2fbf81e8e24da6ed19",
      "30c47f0435084a9d86cf5024cf392ba0",
      "84d2799c14344300a858fabddecc1e6a",
      "e327aaaa4a5d47f0baeab60095540390",
      "6050c7ac77c1432985eb4734ef0f55db",
      "1ccdf51987744ed79c7bc3a2e9583039",
      "22bb0bb5ca5b44d2a579095267b28a97",
      "28237db367184192a8d241ebfe631764",
      "d8fb96b42d0848f4861fc7ed5f6a1037",
      "a6691fd65fb44af490fa372b14299b7c",
      "54c1e20eb35941f0a36e0a7892a56d1d",
      "55d2153f5c3b4e7a816ccd6368b5d853",
      "a1e10bc3f91e408180adc65f30d8939f",
      "21d5c0651627411d83f37e1183d94680",
      "fa9701a43d784d69a84e7d0d445da3ba",
      "a6d9f3034efd4551b346d6dca802d913"
     ]
    },
    "id": "VcNJ3wh7fLJs",
    "outputId": "381bb875-c7d9-449e-8ba2-0d0117679259"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "BLOCK_SIZE = 256  # each training sample will be this many tokens long\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    # convert text into token IDs\n",
    "    return tokenizer(batch[\"text\"], truncation=False)\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "def group_texts(examples):\n",
    "    # glue all tokens together, then cut into fixed-size chunks (BLOCK_SIZE)\n",
    "    concatenated = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated[\"input_ids\"])\n",
    "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    result = {\n",
    "        k: [t[i:i+BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    # labels are the same as input_ids (for next-token prediction)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# final dataset ready for language model training\n",
    "lm_datasets = tokenized.map(group_texts, batched=True)\n",
    "lm_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7erapY5jfOO1"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3KU2oA_fPq0",
    "outputId": "17e77844-8cfd-429d-8564-1d05a1b9baec"
   },
   "outputs": [],
   "source": [
    "# Custom PyTorch training loop\n",
    "import math, os, shutil, time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# need: model, tokenizer, lm_datasets, data_collator (already created above)\n",
    "# hyperparameters (same idea as TrainingArguments)\n",
    "OUTPUT_DIR = \"/content/model_5000cmd_distilgpt2\"\n",
    "per_device_train_batch_size = 2\n",
    "per_device_eval_batch_size  = 2\n",
    "gradient_accumulation_steps = 8\n",
    "num_train_epochs = 2\n",
    "learning_rate = 5e-5\n",
    "weight_decay  = 0.01\n",
    "warmup_ratio  = 0.05\n",
    "logging_steps = 50\n",
    "save_total_limit = 2    # only keep the last 2 checkpoints\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    lm_datasets[\"train\"],\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    lm_datasets[\"validation\"],\n",
    "    batch_size=per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# AdamW optimizer + learning rate scheduler (linear warmup + decay)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / gradient_accumulation_steps)\n",
    "max_train_steps = num_update_steps_per_epoch * num_train_epochs\n",
    "warmup_steps = int(max_train_steps * warmup_ratio)\n",
    "\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return max(0.0, float(max_train_steps - current_step) / float(max(1, max_train_steps - warmup_steps)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# AMP scaler (mixed precision training on GPU if available)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "global_step = 0\n",
    "\n",
    "def evaluate():\n",
    "    # run validation loop and return average loss + perplexity\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            tokens = batch[\"input_ids\"].numel()\n",
    "            total_loss += loss.item() * tokens\n",
    "            total_tokens += tokens\n",
    "    model.train()\n",
    "    avg_loss = total_loss / max(1, total_tokens)\n",
    "    ppl = math.exp(avg_loss) if avg_loss < 20 else float(\"inf\")\n",
    "    return avg_loss, ppl\n",
    "\n",
    "# training loop\n",
    "start_time = time.time()\n",
    "for epoch in range(1, num_train_epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % logging_steps == 0:\n",
    "                curr_lr = scheduler.get_last_lr()[0]\n",
    "                print(f\"[epoch {epoch}] step {global_step}/{max_train_steps} \"\n",
    "                      f\"loss={running_loss:.4f} lr={curr_lr:.6f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    # evaluate + save checkpoint at the end of each epoch\n",
    "    eval_loss, eval_ppl = evaluate()\n",
    "    print(f\"⭐ EVAL epoch {epoch}: eval_loss={eval_loss:.6f}, perplexity={eval_ppl:.2f}\")\n",
    "\n",
    "    ckpt_dir = Path(OUTPUT_DIR) / f\"checkpoint-epoch-{epoch}\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(ckpt_dir)\n",
    "    tokenizer.save_pretrained(ckpt_dir)\n",
    "\n",
    "    # clean up old checkpoints (keep only the last N)\n",
    "    ckpts = sorted(Path(OUTPUT_DIR).glob(\"checkpoint-epoch-*\"), key=os.path.getmtime)\n",
    "    if len(ckpts) > save_total_limit:\n",
    "        for p in ckpts[:-save_total_limit]:\n",
    "            shutil.rmtree(p, ignore_errors=True)\n",
    "\n",
    "# save the final model and tokenizer\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# final eval on validation set and print results\n",
    "final_loss, final_ppl = evaluate()\n",
    "print({\"eval_loss\": final_loss, \"perplexity\": final_ppl})\n",
    "print(\"✅ Saved to\", OUTPUT_DIR, \" | elapsed: %.1f min\" % ((time.time()-start_time)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSW1JU3LfRiM",
    "outputId": "e9ace8c5-a622-411e-e27a-0e8cecc58fe0"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=OUTPUT_DIR,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "prompt = \"In automata theory, a deterministic finite automaton\"\n",
    "out = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=120,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(out)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
